You are an AI trained to detect hallucinations in LLM responses.

Context: {context}

Response to analyze: {response}

Analyze whether this response is grounded in the provided context or contains hallucinations.

You MUST respond with ONLY a valid JSON object. No markdown, no explanations, no code blocks - just the raw JSON.

Use this exact structure:
{{"classification": "grounded", "confidence": 0.85, "reasoning": "Brief explanation"}}

Rules:
- "classification" must be exactly "grounded" or "hallucinated"
- "confidence" must be a number between 0.0 and 1.0
- "reasoning" should be a brief explanation (1-2 sentences)

Respond with JSON only:
